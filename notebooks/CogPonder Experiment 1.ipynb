{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: A Model-Agnostic Approach to Response Times in Cognitive Tasks\n",
    "\n",
    "Of our interest is a simultaneous model of speed and accuracy of response. Yet, most advanced techniques in machine learning cannot capture such a duality of decision making.\n",
    "\n",
    "Inspired by [PonderNet](https://arxiv.org/abs/2107.05407), this notebook demonstrate CogPonder architecture, a model-agnostic architecture that optimizes for the speed-accuracy of human-like responses.\n",
    "\n",
    "The architecture iterates over a recurrent model but only terminates and produce output once it reaches a halting point in time.\n",
    "\n",
    "\n",
    "Unlike sequential sampling and accumulative models, all the parameters of this model *seem* cognitively interpretable.\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "### N-back Task\n",
    "\n",
    "The subject is presented with a sequence of stimuli, and the task consists of indicating when the current stimulus matches the one from n steps earlier in the sequence. The load factor n can be adjusted to make the task more or less difficult.\n",
    "\n",
    "We use a mock N-back dataset to evaluate the architecture. For each subject, the mock N-back dataset includes trial-level $X$, $responses$, $targets$, and $response\\_times$.\n",
    "\n",
    "\n",
    "### Decision Model\n",
    "We want to learn a supervised model of the function $X \\to y$ as follows:\n",
    "\n",
    "$\n",
    "f: X,h_n \\mapsto \\tilde{y},h_{n+1}, \\lambda_n\n",
    "$\n",
    "\n",
    "where $X$ and $y$ denote recent stimulus and responses, $\\lambda_n$ is the halting probability at step $n$, and $h_{n}$ is the latent state of the model. The learning process continues for a maximum of $N_{max}$ steps. For brevity, each step is considered 100ms. - Halting probability ($\\lambda_n$).\n",
    "\n",
    "\n",
    "In case of the N-back task, X is a series of recent N stimuli symbols, e.g., A, B, C, D, ..., $y$ is either TARGET or NON_TARGET.\n",
    "\n",
    "### Output\n",
    "\n",
    "The *PonderNet* model produces $y\\_steps$, $p\\_halts$, $halt\\_steps$ for each item in the batch.\n",
    "\n",
    "\n",
    "### Criterion\n",
    "\n",
    "$L = L_{cross\\_entropy} + L_{target} + L_{non\\_target}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from cogponder import NBackDataset, PonderNet, ICOM, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate mock n-back data\n",
    "\n",
    "n_stimuli = 6\n",
    "\n",
    "dataset = NBackDataset(n_subjects=2, n_trials=100, n_stimuli=6)\n",
    "\n",
    "X, responses, targets, response_times = dataset[0]\n",
    "dataset = TensorDataset(X, responses, targets.float(), response_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03518509864807129,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epochs",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57415a029aa4db794d9150fddcd38aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "MAX_STEPS = 20\n",
    "N_OUTPUTS = torch.unique(targets).size()[0]\n",
    "\n",
    "model = PonderNet(ICOM, n_stimuli+1, n_stimuli, N_OUTPUTS, MAX_STEPS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "evaluate(model, dataset, optimizer, n_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ponder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76027f054aa38c6d5055560f9009a0f692d2c97ebd1740bbb514e2260aad097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
