{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PonderICOM: Joint Modeling of Accuracy and Speed in Cognitive Tasks\n",
    "## Intro\n",
    "\n",
    "In the context of behavioral data, we are interested in simultaneously modeling speed and accuracy. Yet, most advanced techniques in machine learning cannot capture such a duality of decision making data.\n",
    "\n",
    "\n",
    "Building on [PonderNet](https://arxiv.org/abs/2107.05407) and [Variable Rate Coding](https://doi.org/10.32470/CCN.2019.1397-0), this notebook implements a neural model that captures speed and accuracy of human-like responses.\n",
    "\n",
    "Given stimulus symbols as inputs, the model produces two outputs:\n",
    "\n",
    "- Response symbol, which, in comparison with the input stimuli, can be used to measure accuracy).\n",
    "- Halting probability ($\\lambda_n$).\n",
    "\n",
    "Under the hood, the model iterates over a ICOM-like component to reach a halting point in time. Unlike DDM and ICOM models, all the parameters and outcomes of the current model *seem* cognitively interpretable.\n",
    "\n",
    "### Additional resources\n",
    "\n",
    "- [ICOM network model](https://drive.google.com/file/d/16eiUUwKGWfh9pu9VUxzlx046hQNHV0Qe/view?usp=sharinghttps://drive.google.com/file/d/16eiUUwKGWfh9pu9VUxzlx046hQNHV0Qe/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setting\n",
    "\n",
    "### Model\n",
    "Given input and output data, we want to learn a supervised model of the function $X \\to y$ as follows:\n",
    "\n",
    "$\n",
    "f: X,h_n \\mapsto \\tilde{y},h_{n+1}, \\lambda_n\n",
    "$\n",
    "\n",
    "where $X$ and $y$ denote stimulus and response symbols, $\\lambda_n$ denotes halting probability at time $n$, and $h_{n}$ is the latent state of the model. The learninig continious up to the time point $N$.\n",
    "\n",
    "For the brevity and compatibility, both data are one-hot encoded.\n",
    "\n",
    "\n",
    "### Input\n",
    "\n",
    "One-hot encoded symbols.\n",
    "\n",
    "### Output\n",
    "\n",
    "One-hot encoded symbols.\n",
    "\n",
    "### Criterion\n",
    "\n",
    "L = L_cross_entropy + L_halting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "# Setup and imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# import tensorboard as tb\n",
    "# import tensorflow as tf\n",
    "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile #FIX storing embeddings using tensorboard\n",
    "\n",
    "\n",
    "from cogponder import NBackDataset\n",
    "from cogponder import PonderNet, ICOM, ReconstructionLoss, RegularizationLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate mock n-back data\n",
    "\n",
    "n_subjects = 2\n",
    "n_trials = 100\n",
    "n_stimuli = 6\n",
    "\n",
    "dataset = NBackDataset(n_subjects, n_trials, n_stimuli)\n",
    "\n",
    "X, responses, matches, response_times = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1000/1000 [00:36<00:00, 27.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "n_epoches = 1000\n",
    "\n",
    "logs = SummaryWriter()\n",
    "\n",
    "model = PonderNet(ICOM, n_stimuli+1, n_stimuli, 2, 20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = TensorDataset(X, matches.float(), response_times)\n",
    "\n",
    "\n",
    "# split params\n",
    "train_size = int(len(dataset) * .8)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_subset, test_subset = random_split(dataset, lengths=(train_size,test_size))\n",
    "\n",
    "X_train, y_train, rt_train = dataset[train_subset.indices]\n",
    "X_test, y_test, rt_test = dataset[test_subset.indices]\n",
    "\n",
    "loss_rec_fn = ReconstructionLoss(nn.BCELoss(reduction='mean'))\n",
    "loss_reg_fn = RegularizationLoss(lambda_p=.5, max_steps=20)\n",
    "loss_beta = .2\n",
    "\n",
    "loss_fn = nn.BCELoss(reduction='mean')\n",
    "\n",
    "for epoch in tqdm(range(n_epoches), desc='Epochs'):\n",
    "\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  y_steps, p_halt, halt_step = model(X_train)\n",
    "\n",
    "  loss_rec = loss_rec_fn(p_halt, y_steps, y_train)\n",
    "  loss_reg = loss_reg_fn(p_halt, rt_train)\n",
    "  loss = loss_rec + loss_beta * loss_reg\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  logs.add_scalar('loss/rec_train', loss_rec, epoch)\n",
    "  logs.add_scalar('loss/reg_train', loss_reg, epoch)\n",
    "  logs.add_scalar('loss/train', loss, epoch)\n",
    "\n",
    "  y_pred = y_steps.detach()[0, halt_step].argmax(dim=1)\n",
    "  train_accuracy = accuracy_score(y_pred, y_train)\n",
    "  \n",
    "  logs.add_scalar('accuracy/train', train_accuracy, epoch)  \n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    y_steps, p_halt, halt_step = model(X_test)\n",
    "    loss_rec = loss_rec_fn(p_halt, y_steps, y_test)\n",
    "    loss_reg = loss_reg_fn(p_halt, rt_test)\n",
    "    loss = loss_rec + loss_beta * loss_reg\n",
    "    logs.add_scalar('loss/test', loss.detach(), epoch)\n",
    "\n",
    "    y_pred = y_steps.detach()[0, halt_step].argmax(dim=1)\n",
    "    test_accuracy = accuracy_score(y_pred, y_test)\n",
    "  \n",
    "    logs.add_scalar('accuracy/test', test_accuracy, epoch)  \n",
    "\n",
    "# tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('CogPonder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fda182afad02a599cbe29528311e73282b27fcc00ceb6a951541e1119641c8c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
