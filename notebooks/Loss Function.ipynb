{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: N-Back PyTorch Lightning\n",
    "\n",
    "This notebook implements the CogPonder algorithm using PyTorch Lightning. It assumes fixed hyperparameters and fits the model to a single-subject dataset. It wraps a simple RRN with a pondering lambda layer and trains it on the *Self-Regulation Ontology* dataset.\n",
    "\n",
    "## Data\n",
    "The SRO-2back dataset interface provides the following features from the *Self-Regulation Ontology* study:\n",
    "\n",
    "- input `X`: previous 3 symbols for the subject $i$ and trial $j$; For each subject, $X_i$ is a 2-dimensional vector of integers of shape (3, $N_{\\text{trials}}$).\n",
    "- `trial_type`: Correct match, incorrect match, correct-non-match, incorrect-non-match for each trial $i$.\n",
    "- `is_target`: whether the trial $i$ is a match; it is a boolean.\n",
    "- output `response`: the response of the subject for the trial i; it is a boolean.\n",
    "- `response_step`: the response step of the subject for the trial i; Response step is an integer and represents *response times* in 50ms steps. This step duration is a hyperparameter of the data module.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "- `n_embeddings`: number of hidden units in the recurrent ICOM model. Defaults to $N_{\\text{symbols} + 1}$\n",
    "- `rec_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `cog_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `learning_rate`: the learning rate of the optimizer. Defaults to 0.0001.\n",
    "- `max_response_step`: maximum response step in the dataset. Defaults to $\\max(\\text{response\\_step}) + 10$.\n",
    "\n",
    "## Criterion\n",
    "\n",
    "$L = L_{\\text{reconstruction}} + L_{\\text{cognitive}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from ray import tune, air\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from functools import partial\n",
    "from pytorch_lightning.callbacks import RichProgressBar, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from cogponder import CogPonderNet\n",
    "from cogponder.datasets import NBackMockDataset, NBackDataModule, NBackSRODataset\n",
    "from pathlib import Path\n",
    "\n",
    "from cogponder.losses import ReconstructionLoss, CognitiveLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "data = NBackSRODataset(n_subjects=1, n_back=2) # shape (n_subjects, (...))\n",
    "datamodule = NBackDataModule(data, batch_size=32, num_workers=1)\n",
    "n_symbols = torch.unique(data[0][0]).shape[0]\n",
    "max_response_step = 25 # data[0][4].max().item() + 1 # max number of steps = 2 * max observed RT\n",
    "\n",
    "# parameter space\n",
    "CONFIG = {\n",
    "    'rec_loss_beta': 1.,\n",
    "    'cog_loss_beta': .1,\n",
    "    'loss_by_trial_type': False,\n",
    "    'learning_rate': 1e-2,\n",
    "    'max_response_step': max_response_step,\n",
    "    'inputs_dim': data[0][0].size(1),\n",
    "    'embeddings_dim': n_symbols,\n",
    "    'auto_lr_find': False,\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">         In sizes </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Out sizes </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ train_accuracy │ Accuracy           │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_accuracy   │ Accuracy           │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ cog_loss_fn    │ CognitiveLoss      │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ rec_loss_fn    │ ReconstructionLoss │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ halt_node      │ Sequential         │      6 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">           [1, 5] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ output_node    │ Sequential         │      6 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">           [1, 5] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ recurrent_node │ GRUCell            │    150 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [[1, 3], [1, 5]] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 5] </span>│\n",
       "└───┴────────────────┴────────────────────┴────────┴──────────────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m        In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mOut sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ train_accuracy │ Accuracy           │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_accuracy   │ Accuracy           │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ cog_loss_fn    │ CognitiveLoss      │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ rec_loss_fn    │ ReconstructionLoss │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ halt_node      │ Sequential         │      6 │\u001b[37m \u001b[0m\u001b[37m          [1, 5]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m   [1, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ output_node    │ Sequential         │      6 │\u001b[37m \u001b[0m\u001b[37m          [1, 5]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m   [1, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ recurrent_node │ GRUCell            │    150 │\u001b[37m \u001b[0m\u001b[37m[[1, 3], [1, 5]]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m   [1, 5]\u001b[0m\u001b[37m \u001b[0m│\n",
       "└───┴────────────────┴────────────────────┴────────┴──────────────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 162                                                                                              \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 162                                                                                                  \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 162                                                                                              \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 162                                                                                                  \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb91be34085409a9cf564bbe8ec591c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/morteza/mambaforge/envs/ponder/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connect\n",
       "or.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this\n",
       "machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/morteza/mambaforge/envs/ponder/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connect\n",
       "or.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this\n",
       "machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/morteza/mambaforge/envs/ponder/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connect\n",
       "or.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this\n",
       "machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/morteza/mambaforge/envs/ponder/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connect\n",
       "or.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a \n",
       "bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this\n",
       "machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pondering model\n",
    "\n",
    "model = CogPonderNet(CONFIG, example_input_array=data[0][0][:1].to(device))\n",
    "\n",
    "# # DEBUG\n",
    "# X = data[0][0][:10]\n",
    "# y_true = data[0][3][:10]\n",
    "# rt_true = data[0][4][:10]\n",
    "# y_steps, p_halts, rt_pred = model(X)\n",
    "# loss_func = CognitiveLoss(CONFIG['max_response_step'])\n",
    "# l = loss_func(rt_pred, rt_true)\n",
    "# 'l', l, rt_pred, rt_true\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5000,\n",
    "    # min_epochs=1000,\n",
    "    accelerator='auto',\n",
    "    auto_lr_find=CONFIG['auto_lr_find'],\n",
    "    log_every_n_steps=4,\n",
    "    # overfit_batches=True,\n",
    "    # accumulate_grad_batches=4,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor='val_loss', patience=1000, mode='min', min_delta=0.01),\n",
    "    ])\n",
    "\n",
    "if CONFIG['auto_lr_find']:\n",
    "    trainer.tune(model, datamodule=datamodule)\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Plot LR tuning results\n",
    "# lr_finder = trainer.tuner.lr_find(model, max_lr=2, datamodule=datamodule)\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "# model.hparams.learning_rate = lr_finder.suggestion()\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 11\u001b[0m     y_train_steps,_,rt_train_pred \u001b[39m=\u001b[39m model(X_train\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     12\u001b[0m     y_test_steps,_,rt_test_pred \u001b[39m=\u001b[39m model(X_test\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     14\u001b[0m     y_train_pred \u001b[39m=\u001b[39m y_train_steps\u001b[39m.\u001b[39mgather(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, index\u001b[39m=\u001b[39mrt_train_pred[\u001b[39mNone\u001b[39;00m, :] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m,)[\u001b[39m0\u001b[39m]  \u001b[39m# (batch_size,)\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/ponder/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/CogPonder/cogponder/cogpondernet.py:77\u001b[0m, in \u001b[0;36mCogPonderNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m     lambda_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhalt_node(h)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m     76\u001b[0m y_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_node(h)[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m---> 77\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecurrent_node(x, h)\n\u001b[1;32m     79\u001b[0m y_list\u001b[39m.\u001b[39mappend(y_step)\n\u001b[1;32m     80\u001b[0m p_list\u001b[39m.\u001b[39mappend(p_continue \u001b[39m*\u001b[39m lambda_n)\n",
      "File \u001b[0;32m~/mambaforge/envs/ponder/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/ponder/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1279\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[0;32m-> 1279\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru_cell(\n\u001b[1;32m   1280\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[1;32m   1281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[1;32m   1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[1;32m   1283\u001b[0m )\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1286\u001b[0m     ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "X_train, _, y_train, _, rt_train = datamodule.dataset[datamodule.train_dataset.indices]\n",
    "X_test, _, y_test, _, rt_test = datamodule.dataset[datamodule.test_dataset.indices]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_train_steps,_,rt_train_pred = model(X_train.to(device))\n",
    "    y_test_steps,_,rt_test_pred = model(X_test.to(device))\n",
    "\n",
    "    y_train_pred = y_train_steps.gather(dim=0, index=rt_train_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "    y_test_pred = y_test_steps.gather(dim=0, index=rt_test_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "\n",
    "    accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "    train_accuracy = accuracy_fn(y_train_pred, y_train.to(device))\n",
    "    print('TRAIN ACCURACY', train_accuracy.item())\n",
    "\n",
    "\n",
    "    accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "    test_accuracy = accuracy_fn(y_test_pred, y_test.to(device))\n",
    "    print('TEST ACCURACY', test_accuracy.item())\n",
    "\n",
    "    # DEBUG report the ground truth and predicted response times\n",
    "    print('TRUE TRAIN:', rt_train.detach().tolist(), '\\nPRED TRAIN:',  rt_train_pred.tolist())\n",
    "    print('TRUE TEST:', rt_test.detach().tolist(), '\\nPRED TEST:',  rt_test_pred.tolist())\n",
    "\n",
    "# DEBUG report medians\n",
    "# rt_train_pred.median(), rt_train.float().median()\n",
    "# rt_test_pred.median(), rt_test.float().median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RT_CAP = max_response_step # data[0][4].max().item()\n",
    "\n",
    "sns.ecdfplot(rt_train, label='True (train)')\n",
    "sns.ecdfplot(rt_train_pred[rt_train_pred < RT_CAP], label='Predicted (train)')\n",
    "\n",
    "plt.title('Evaluation of PonderNet on simulated train split')\n",
    "plt.xlabel('response time (steps)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.ecdfplot(rt_test, label='True (test)')\n",
    "sns.ecdfplot(rt_test_pred[rt_test_pred < RT_CAP], label='Predicted (test)')\n",
    "\n",
    "plt.title('Evaluation of PonderNet on simulated test split')\n",
    "plt.xlabel('response time (steps)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.kdeplot(rt_train, label='Train (TRUE)', cut=0)\n",
    "sns.kdeplot(rt_train_pred[rt_train_pred < RT_CAP], label='Train (PRED)', cut=0)\n",
    "\n",
    "sns.kdeplot(rt_test, label='Test (TRUE)', cut=0)\n",
    "sns.kdeplot(rt_test_pred[rt_test_pred < RT_CAP], label='Test (PRED)', cut=0)\n",
    "\n",
    "\n",
    "plt.title('Evaluation of PonderNet on SRO single-subject 2-back')\n",
    "plt.xlabel('response time (steps)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ponder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70f45b4cee24464c60504a5fe56f777ef44c770208b66a1d7d40e1cf1ca2ecf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
