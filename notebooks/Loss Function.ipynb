{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: N-Back PyTorch Lightning\n",
    "\n",
    "This notebook implements the CogPonder algorithm using PyTorch Lightning. It assumes fixed hyperparameters and fits the model to a single-subject dataset. It wraps a simple RRN with a pondering lambda layer and trains it on the *Self-Regulation Ontology* dataset.\n",
    "\n",
    "## Data\n",
    "The SRO-2back dataset interface provides the following features from the *Self-Regulation Ontology* study:\n",
    "\n",
    "- `X`: previous 3 symbols for the subject $i$ and trial $j$; For each subject, $X_i$ is a 2-dimensional vector of integers of shape (3, $N_{\\text{trials}}$).\n",
    "- `trial_types`: Correct match, incorrect match, correct-non-match, incorrect-non-match for each trial $i$.\n",
    "- `is_targets`: whether the trial $i$ is a match; it is a boolean.\n",
    "- `response`: the response of the subject for the trial i; it is a boolean.\n",
    "- `response_steps`: the response step of the subject for the trial i; Response step is an integer and represents *response times* in 50ms steps. This step duration is a hyperparameter of the data module.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "- `n_embeddings`: number of hidden units in the recurrent ICOM model. Defaults to $N_{\\text{symbols} + 1}$\n",
    "- `rec_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `cog_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `learning_rate`: the learning rate of the optimizer. Defaults to 0.0001.\n",
    "- `max_response_step`: maximum response step in the dataset. Defaults to $\\max(\\text{response\\_step}) + 10$.\n",
    "\n",
    "## Criterion\n",
    "\n",
    "$L = L_{\\text{reconstruction}} + L_{\\text{cognitive}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from ray import tune, air\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from functools import partial\n",
    "from pytorch_lightning.callbacks import RichProgressBar, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from cogponder import CogPonderNet\n",
    "from cogponder.datasets import NBackMockDataset, NBackDataModule, NBackSRODataset\n",
    "from pathlib import Path\n",
    "\n",
    "from cogponder.losses import ReconstructionLoss, CognitiveLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "data = NBackSRODataset(n_subjects=1, n_back=2) # shape (n_subjects, (...))\n",
    "datamodule = NBackDataModule(data, batch_size=256, num_workers=1)\n",
    "n_symbols = torch.unique(data[0][0]).shape[0]\n",
    "max_response_step = 30  # data[0][4].max().item() * 2  # max number of steps = 2 * max observed RT\n",
    "\n",
    "# parameter space\n",
    "CONFIG = {\n",
    "    'rec_loss_beta': 1.,\n",
    "    'cog_loss_beta': .3,\n",
    "    'loss_by_trial_type': False,\n",
    "    'learning_rate': 1e-3,\n",
    "    'max_response_step': max_response_step,\n",
    "    'inputs_dim': data[0][0].size(1),\n",
    "    'embeddings_dim': n_symbols,\n",
    "}\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# pondering model\n",
    "\n",
    "model = CogPonderNet(CONFIG, example_input_array=data[0][0][:1].to('cuda'))\n",
    "\n",
    "# # DEBUG\n",
    "# X = data[0][0][:10]\n",
    "# y_true = data[0][3][:10]\n",
    "# rt_true = data[0][4][:10]\n",
    "# y_steps, p_halts, rt_pred = model(X)\n",
    "# loss_func = CognitiveLoss(CONFIG['max_response_step'])\n",
    "# l = loss_func(rt_pred, rt_true)\n",
    "# l\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10000,\n",
    "    accelerator='auto',\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    log_every_n_steps=8,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor='val_loss', patience=1000, mode='min', min_delta=0.01),\n",
    "    ])\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "X_train, _, y_train, _, rt_train = datamodule.dataset[datamodule.train_dataset.indices]\n",
    "X_test, _, y_test, _, rt_test = datamodule.dataset[datamodule.test_dataset.indices]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_train_steps,_,rt_train_pred = model(X_train)\n",
    "    y_test_steps,_,rt_test_pred = model(X_test)\n",
    "\n",
    "    y_train_pred = y_train_steps.gather(dim=0, index=rt_train_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "    y_test_pred = y_test_steps.gather(dim=0, index=rt_test_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "\n",
    "    accuracy_fn = torchmetrics.Accuracy()\n",
    "    train_accuracy = accuracy_fn(y_train_pred, y_train)\n",
    "    print('TRAIN ACCURACY', train_accuracy.item())\n",
    "\n",
    "\n",
    "    accuracy_fn = torchmetrics.Accuracy()\n",
    "    test_accuracy = accuracy_fn(y_test_pred, y_test)\n",
    "    print('TEST ACCURACY', test_accuracy.item())\n",
    "\n",
    "    # DEBUG report the ground truth and predicted response times\n",
    "    print('TRUE TRAIN:', rt_train.detach().tolist(), '\\nPRED TRAIN:',  rt_train_pred.tolist())\n",
    "    print('TRUE TEST:', rt_test.detach().tolist(), '\\nPRED TEST:',  rt_test_pred.tolist())\n",
    "\n",
    "# DEBUG report medians\n",
    "# rt_train_pred.median(), rt_train.float().median()\n",
    "# rt_test_pred.median(), rt_test.float().median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.ecdfplot(rt_train, label='True (train)')\n",
    "sns.ecdfplot(rt_train_pred[rt_train_pred < max_response_step], label='Predicted (train)')\n",
    "\n",
    "plt.title('Evaluation of PonderNet on simulated train split')\n",
    "plt.xlabel('response time (steps)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.ecdfplot(rt_test, label='True (test)')\n",
    "sns.ecdfplot(rt_test_pred[rt_test_pred < max_response_step], label='Predicted (test)')\n",
    "\n",
    "plt.title('Evaluation of PonderNet on simulated test split')\n",
    "plt.xlabel('response time (steps)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.kdeplot(rt_train, label='Train (TRUE)', cut=0)\n",
    "sns.kdeplot(rt_train_pred[rt_train_pred < max_response_step], label='Train (PRED)', cut=0)\n",
    "\n",
    "sns.kdeplot(rt_test, label='Test (TRUE)', cut=0)\n",
    "sns.kdeplot(rt_test_pred[rt_test_pred < max_response_step], label='Test (PRED)', cut=0)\n",
    "\n",
    "\n",
    "plt.title('Evaluation of PonderNet on SRO single-subject 2-back')\n",
    "plt.xlabel('response time (steps)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ponder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70f45b4cee24464c60504a5fe56f777ef44c770208b66a1d7d40e1cf1ca2ecf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
