{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: N-Back PyTorch Lightning\n",
    "\n",
    "This notebook implements the CogPonder algorithm using PyTorch Lightning. It assumes fixed hyperparameters and fits the model to a single-subject dataset. It wraps a simple RRN with a pondering lambda layer and trains it on the *Self-Regulation Ontology* dataset.\n",
    "\n",
    "## Data\n",
    "\n",
    "Either N-back or Stroop is used as the dataset. The data is loaded from the `data/Self_Regulation_ontology/` directory.\n",
    "\n",
    "### Input and Output\n",
    "\n",
    "#### N-Back\n",
    "\n",
    "Previous N+1 presented symbols are used as input, the last input is the current symbol. The output is the human response to the N+1th trial.\n",
    "\n",
    "#### Stroop\n",
    "\n",
    "The input is the color and letter of the current stimuli. The output is the human response to the current trial.\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "- `n_embeddings`: number of hidden units in the recurrent ICOM model. Defaults to $N_{\\text{symbols} + 1}$\n",
    "- `rec_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `cog_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `learning_rate`: the learning rate of the optimizer. Defaults to 0.0001.\n",
    "- `max_response_step`: maximum response step in the dataset. Defaults to $\\max(\\text{response\\_step}) + 10$.\n",
    "\n",
    "## Criterion\n",
    "\n",
    "$L = L_{\\text{reconstruction}} + L_{\\text{cognitive}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from ray import tune, air\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from functools import partial\n",
    "from pytorch_lightning.callbacks import RichProgressBar, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from cogponder import CogPonderNet\n",
    "from cogponder.datasets import StroopSRODataset, NBackSRODataset, CogPonderDataModule\n",
    "from pathlib import Path\n",
    "\n",
    "from cogponder.losses import ReconstructionLoss, CognitiveLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset and configs\n",
    "\n",
    "TASK = 'stroop'  # or stroop\n",
    "\n",
    "match TASK:\n",
    "    case 'nback':\n",
    "        print('Loading N-back dataset...')\n",
    "        data = NBackSRODataset(n_subjects=1, n_back=2) # shape (n_subjects, (...))\n",
    "        n_symbols = torch.unique(data[0][0]).shape[0]\n",
    "        embeddings_dim = n_symbols\n",
    "        max_response_step = 30 # OR something like \"2 * max observed RT\"\n",
    "    case 'stroop':\n",
    "        print('Loading Stroop dataset...')\n",
    "        data = StroopSRODataset(n_subjects=1)\n",
    "        embeddings_dim = 2\n",
    "        max_response_step = 100\n",
    "\n",
    "# parameter space\n",
    "CONFIG = {\n",
    "    'rec_loss_beta': 1.,\n",
    "    'cog_loss_beta': 1.,\n",
    "    'loss_by_trial_type': False,\n",
    "    'learning_rate': 1e-3,\n",
    "    'max_response_step': max_response_step,\n",
    "    'inputs_dim': data[0][0].size(1),\n",
    "    'embeddings_dim': embeddings_dim,\n",
    "    'auto_lr_find': False,\n",
    "    'task': TASK,\n",
    "    'batch_size': 196\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# pondering model\n",
    "\n",
    "datamodule = CogPonderDataModule(data, batch_size=CONFIG['batch_size'], num_workers=1)\n",
    "model = CogPonderNet(CONFIG, example_input_array=data[0][0][:1].to(device))\n",
    "\n",
    "# # DEBUG\n",
    "# X = data[0][0][:10]\n",
    "# y_true = data[0][3][:10]\n",
    "# rt_true = data[0][4][:10]\n",
    "# y_steps, p_halts, rt_pred = model(X)\n",
    "# loss_func = CognitiveLoss(CONFIG['max_response_step'])\n",
    "# l = loss_func(rt_pred, rt_true)\n",
    "# 'l', l, rt_pred, rt_true\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100000,\n",
    "    # min_epochs=1000,\n",
    "    accelerator='auto',\n",
    "    auto_lr_find=CONFIG['auto_lr_find'],\n",
    "    log_every_n_steps=1,\n",
    "    overfit_batches=True,\n",
    "    # accumulate_grad_batches=2,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        # EarlyStopping(monitor='val_loss', patience=2000, mode='min', min_delta=0.001),\n",
    "    ])\n",
    "\n",
    "if CONFIG['auto_lr_find']:\n",
    "    trainer.tune(model, datamodule=datamodule)\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Plot LR tuning results\n",
    "# lr_finder = trainer.tuner.lr_find(model, max_lr=2, datamodule=datamodule)\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "# model.hparams.learning_rate = lr_finder.suggestion()\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "X_train, _, y_train, rt_train = datamodule.dataset[datamodule.train_dataset.indices]\n",
    "X_test, _, y_test, rt_test = datamodule.dataset[datamodule.test_dataset.indices]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_steps,_,rt_train_pred = model(X_train.to(device))\n",
    "    y_train_pred = y_train_steps.gather(dim=0, index=rt_train_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "    y_test_pred = y_test_steps.gather(dim=0, index=rt_test_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "\n",
    "    accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "    train_accuracy = accuracy_fn(y_train_pred, y_train.int().to(device))\n",
    "    print('TRAIN ACCURACY', train_accuracy.item())\n",
    "\n",
    "\n",
    "    accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "    test_accuracy = accuracy_fn(y_test_pred, y_test.int().to(device))\n",
    "    print('TEST ACCURACY', test_accuracy.item())\n",
    "\n",
    "    # DEBUG report the ground truth and predicted response times\n",
    "    print('TRUE TRAIN:', rt_train.detach().tolist(), '\\nPRED TRAIN:',  rt_train_pred.tolist())\n",
    "    print('TRUE TEST:', rt_test.detach().tolist(), '\\nPRED TEST:',  rt_test_pred.tolist())\n",
    "\n",
    "# DEBUG report medians\n",
    "# rt_train_pred.median(), rt_train.float().median()\n",
    "# rt_test_pred.median(), rt_test.float().median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RT_CAP = max_response_step # data[0][4].max().item()\n",
    "\n",
    "sns.ecdfplot(rt_train, label='True (train)')\n",
    "sns.ecdfplot(rt_train_pred[rt_train_pred < RT_CAP].cpu(), label='Predicted (train)')\n",
    "\n",
    "plt.title('Evaluation of PonderNet on simulated train split')\n",
    "plt.xlabel('response time (steps)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.ecdfplot(rt_test, label='True (test)')\n",
    "sns.ecdfplot(rt_test_pred[rt_test_pred < RT_CAP].cpu(), label='Predicted (test)')\n",
    "\n",
    "plt.title('Evaluation of PonderNet on simulated test split')\n",
    "plt.xlabel('response time (steps)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.kdeplot(rt_train, label='Train (TRUE)', cut=0)\n",
    "sns.kdeplot(rt_train_pred[rt_train_pred < RT_CAP].cpu(), label='Train (PRED)', cut=0)\n",
    "\n",
    "sns.kdeplot(rt_test, label='Test (TRUE)', cut=0)\n",
    "sns.kdeplot(rt_test_pred[rt_test_pred < RT_CAP].cpu(), label='Test (PRED)', cut=0)\n",
    "\n",
    "\n",
    "plt.title('Evaluation of PonderNet on SRO single-subject 2-back')\n",
    "plt.xlabel('response time (steps)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ponder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70f45b4cee24464c60504a5fe56f777ef44c770208b66a1d7d40e1cf1ca2ecf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
