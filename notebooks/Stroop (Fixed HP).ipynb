{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: Stroop task with fixed hyper-parameters\n",
    "\n",
    "This notebook implements the CogPonder framework using PyTorch Lightning and evaluate it on the Stroop task.\n",
    "\n",
    "\n",
    "It assumes fixed hyper-parameters and fits the model to a single-subject dataset. It wraps a simple linear network with a pondering layer and trains it on a random subject from the *Self-Regulation Ontology* dataset.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "Here, we fit the Stroop data from the *Self-Regulation Ontology*. The data is loaded from the `data/Self_Regulation_ontology/` directory; see the corresponding [README](../data/Self_Regulation_Ontology/README.md) for more information on the data structure.\n",
    "\n",
    "### Input and Output\n",
    "\n",
    "\n",
    "The input is a list of trials, including 1) the color and 2) the letter of the current stimulus. The output is the human response to the current trial (i.e., red, green, or blue), and response time.\n",
    "\n",
    "\n",
    "## Hyper-parameters\n",
    "\n",
    "- `n_embeddings`: number of hidden units in the operator model. Defaults to 8.\n",
    "- `resp_loss_beta`: the beta parameter of the loss function. Defaults to 1.\n",
    "- `time_loss_beta`: the beta parameter of the loss function. Defaults to 10.\n",
    "- `learning_rate`: the learning rate of the optimizer. Defaults to 0.001.\n",
    "- `max_response_step`: maximum response step. Defaults to the maximum response time in the dataset divided by the response_step_interval.\n",
    "- `response_step_interval`: the interval between response steps. Defaults to 10ms.\n",
    "\n",
    "## Criterion\n",
    "\n",
    "The loss function is a weighted sum of the reconstruction loss ($L_{\\text{response}}$) which measures the corss-entropy loss between human response and predicted response, and is regularized by the response time loss ($L_{\\text{time}}$), which measures the KL-div between human response times and predicted response times.\n",
    "\n",
    "$L_{\\text{total}} = \\sum pL_{\\text{response}} + \\beta L_{\\text{time}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\n",
    "from src.cogponder import CogPonderModel\n",
    "from src.cogponder.datasets import StroopSRODataset, CogPonderDataModule\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "TASK = 'stroop'\n",
    "\n",
    "# this notebook only fits one SRO subject, which its SRO-SubjectID can be defined here\n",
    "SRO_SUBJECT_ID = 202\n",
    "\n",
    "# number of maximum epochs to train (early stopping will be applied)\n",
    "# early stopping patience is 10% of max_epochs (min 10 epochs)\n",
    "MAX_EPOCHS = 10000\n",
    "\n",
    "# upon successful training, the trained model will be saved to this path\n",
    "CHECKPOINT_PATH = (\n",
    "    Path('models') / TASK / f'cogponder_subject-{SRO_SUBJECT_ID}_epochs-embeddings-8_{MAX_EPOCHS}.ckpt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset and configs\n",
    "\n",
    "print(f'Loading {TASK} dataset... ', end='')\n",
    "\n",
    "dataset = StroopSRODataset(n_subjects=-1, response_step_interval=10, non_decision_time='auto')\n",
    "data = dataset[SRO_SUBJECT_ID]\n",
    "\n",
    "# determine the number of loaded subjects\n",
    "n_subjects = data[:][0].size(1)\n",
    "\n",
    "# parameter space\n",
    "CONFIG = {\n",
    "    'task': TASK,\n",
    "    'resp_loss_beta': 1.,\n",
    "    'time_loss_beta': 10.,\n",
    "    # 'non_decision_time': 10,  # in milliseconds\n",
    "    'loss_by_trial_type': False,\n",
    "    'learning_rate': 1e-2,\n",
    "    'max_response_step': data[4].max().int().item() + 10,\n",
    "    'inputs_dim': data[0].size(1) - 1,  # minus subject_id (first column)\n",
    "    'embeddings_dim': 8,\n",
    "    'outputs_dim': torch.unique(data[3]).size(0),  # number of unique responses\n",
    "    'auto_lr_find': False,\n",
    "    'batch_size': 36,\n",
    "    'n_subjects': 1\n",
    "}\n",
    "\n",
    "datamodule = CogPonderDataModule(data,\n",
    "                                 batch_size=CONFIG['batch_size'],\n",
    "                                 num_workers=8)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # or 'mps' on Mx Macs\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Define the pondering model and run the trainer\n",
    "\n",
    "model = CogPonderModel(CONFIG)#, example_input_array=data[0][:1].to(device))\n",
    "\n",
    "# TODO: check if torch>=2.0 is installed\n",
    "# model = torch.compile(model)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    # min_epochs=100,\n",
    "    accelerator='auto',\n",
    "    auto_lr_find=CONFIG['auto_lr_find'],\n",
    "    log_every_n_steps=1,\n",
    "    # overfit_batches=True,\n",
    "    # accumulate_grad_batches=2,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor='val/total_loss',\n",
    "                      patience=np.max([10, MAX_EPOCHS // 10]).item(),\n",
    "                      mode='min', min_delta=0.001),\n",
    "    ])\n",
    "\n",
    "# Auto-detect learning-rate if the flag is set\n",
    "if CONFIG['auto_lr_find']:\n",
    "    trainer.tune(model, datamodule=datamodule)\n",
    "\n",
    "# Fit and evaluate the model\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "# Save the latest checkpoint\n",
    "trainer.save_checkpoint(CHECKPOINT_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "# DEBUG - Load the checkpoint\n",
    "\n",
    "model_ckpt = CogPonderModel.load_from_checkpoint(CHECKPOINT_PATH)\n",
    "model_ckpt.eval()\n",
    "\n",
    "# reload the data module if it is not loaded yet\n",
    "if not 'datamodule' in locals() or not hasattr(datamodule, 'train_dataset'):\n",
    "    print('loading data module...', end='')\n",
    "    data = StroopSRODataset(n_subjects=-1, response_step_interval=10)[SRO_SUBJECT_ID]\n",
    "    datamodule = CogPonderDataModule(data, batch_size=CONFIG['batch_size'], num_workers=8)\n",
    "    datamodule.prepare_data()\n",
    "    print('Done!')\n",
    "\n",
    "X_train, trial_types_train, is_corrects_train, y_train, rt_train = datamodule.train_dataset[:]\n",
    "X_test, trial_types_test, is_corrects_test, y_test, rt_test = datamodule.test_dataset[:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_ckpt.eval()\n",
    "\n",
    "    y_train_steps,p_train,rt_train_pred = model_ckpt(X_train)\n",
    "    y_test_steps,p_test,rt_test_pred = model_ckpt(X_test)\n",
    "\n",
    "    y_train_steps = torch.argmax(y_train_steps, dim=-1)\n",
    "    y_test_steps = torch.argmax(y_test_steps, dim=-1)\n",
    "\n",
    "    y_train_pred = y_train_steps.gather(dim=0, index=rt_train_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "    y_test_pred = y_test_steps.gather(dim=0, index=rt_test_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "\n",
    "    train_res = pd.DataFrame({'true_rt_train': rt_train.detach().tolist(),\n",
    "                              'pred_rt_train': rt_train_pred.tolist()})\n",
    "    test_res = pd.DataFrame({'true_rt_test': rt_test.detach().tolist(),\n",
    "                             'pred_rt_test': rt_test_pred.tolist()})\n",
    "\n",
    "    display(train_res.T, test_res.T)\n",
    "\n",
    "    # report Stroop accuracy\n",
    "    is_corrects_pred = (y_test_pred.long() == y_test).float()\n",
    "    cong_is_corrects = torch.where(trial_types_test == 1, is_corrects_pred, torch.nan)\n",
    "    incong_is_corrects = torch.where(trial_types_test == 0, is_corrects_pred, torch.nan)\n",
    "\n",
    "    accuracy = torch.nanmean(is_corrects_pred)\n",
    "    cong_accuracy = torch.nanmean(cong_is_corrects)\n",
    "    incong_accuracy = torch.nanmean(incong_is_corrects)\n",
    "\n",
    "    print(TASK, '%correct (total / congruent / incongruent): {:.3f} / {:.3f} / {:.3f}'.format(\n",
    "            accuracy.item(), cong_accuracy.item(), incong_accuracy.item()))\n",
    "\n",
    "# DEBUG report mean-RT\n",
    "print(f'RT train mean (pred/true): '\n",
    "      f'{rt_train_pred.float().mean().item():.2f}, '\n",
    "      f'{rt_train.float().mean().item():.2f}')\n",
    "\n",
    "print(f'RT test  mean (pred/true): '\n",
    "      f'{rt_test_pred.float().mean().item():.2f}, '\n",
    "      f'{rt_test.float().mean().item():.2f}')\n",
    "\n",
    "# DEBUG - report sd-RT\n",
    "print(f'RT train std (pred/true): '\n",
    "      f'{rt_train_pred.float().std().item():.2f}, '\n",
    "      f'{rt_train.float().std().item():.2f}')\n",
    "\n",
    "print(f'RT test  std (pred/true): '\n",
    "      f'{rt_test_pred.float().std().item():.2f}, '\n",
    "      f'{rt_test.float().std().item():.2f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predicted response time distributions per condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# prepare plotting data\n",
    "\n",
    "plot_data_train = pd.DataFrame({\n",
    "    'rt': rt_train,\n",
    "    'trial_type': trial_types_train,\n",
    "    'color': X_train[:, 0],\n",
    "    'word': X_train[:, 1],\n",
    "    'data': 'truth',\n",
    "    'split': 'train',\n",
    "})\n",
    "\n",
    "plot_data_train_pred = pd.DataFrame({\n",
    "    'rt': rt_train_pred,\n",
    "    'trial_type': trial_types_train,\n",
    "    'color': X_train[:, 0],\n",
    "    'word': X_train[:, 1],\n",
    "    'data': 'pred',\n",
    "    'split': 'train',\n",
    "})\n",
    "\n",
    "plot_data_test = pd.DataFrame({\n",
    "    'rt': rt_test,\n",
    "    'trial_type': trial_types_test,\n",
    "    'color': X_test[:, 0],\n",
    "    'word': X_test[:, 1],\n",
    "    'data': 'truth',\n",
    "    'split': 'test',\n",
    "})\n",
    "\n",
    "plot_data_test_pred = pd.DataFrame({\n",
    "    'rt': rt_test_pred,\n",
    "    'trial_type': trial_types_test,\n",
    "    'color': X_test[:, 0],\n",
    "    'word': X_test[:, 1],\n",
    "    'data': 'pred',\n",
    "    'split': 'test',\n",
    "})\n",
    "\n",
    "# merge and cleanup\n",
    "plot_data = pd.concat([plot_data_train, plot_data_train_pred, plot_data_test, plot_data_test_pred])\n",
    "plot_data['trial_type'] = plot_data['trial_type'].map({0: 'incongruent', 1: 'congruent'})\n",
    "\n",
    "max_rt = CONFIG[\"max_response_step\"]\n",
    "\n",
    "plot_data = plot_data.query('rt < @max_rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "g = sns.displot(\n",
    "    plot_data,\n",
    "    x='rt', row='data', col='split', hue='trial_type',\n",
    "    kind='ecdf', linewidth=2, height=3,\n",
    ")\n",
    "\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "# plt.title(f'Evaluation profile of CogPonder on train split (Stroop)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the overall predicted response time distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.displot(\n",
    "    plot_data.reset_index(),\n",
    "    x='rt', row='data', col='split', #hue='trial_type',\n",
    "    kind='hist', height=3, #palette='Greens',\n",
    "    kde=True, kde_kws={'clip': (0, max_rt)},\n",
    "    facet_kws={'sharey': False}\n",
    ")\n",
    "# sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "# plt.title(f'Evaluation profile of CogPonder on train split (Stroop)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ponder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:27:35) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76027f054aa38c6d5055560f9009a0f692d2c97ebd1740bbb514e2260aad097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
