{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: Stroop task with fixed hyper-parameters\n",
    "\n",
    "This notebook implements the CogPonder framework using PyTorch Lightning and evaluate it on the Stroop task.\n",
    "\n",
    "\n",
    "It assumes fixed hyper-parameters and fits the model to a single-subject dataset. It wraps a simple linear network with a pondering layer and trains it on the *Self-Regulation Ontology* dataset.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "Here, we fit the Stroop data from the *Self-Regulation Ontology*. The data is loaded from the `data/Self_Regulation_ontology/` directory; see the [README](../data/Self_Regulation_Ontology/README.md) for more information on the data structure.\n",
    "\n",
    "### Input and Output\n",
    "\n",
    "\n",
    "The input contains a list of trials, each includes 1) the color and 2) the letter of the current stimulus. The output is the human response to the current trial (i.e., red, green, or blue).\n",
    "\n",
    "\n",
    "## Hyper-parameters\n",
    "\n",
    "- `n_embeddings`: number of hidden units in the operator model. Defaults to 2.\n",
    "- `resp_loss_beta`: the beta parameter of the loss function. Defaults to 1.\n",
    "- `time_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `learning_rate`: the learning rate of the optimizer. Defaults to 0.0001.\n",
    "- `max_response_step`: maximum response step in the dataset. Defaults to 100.\n",
    "\n",
    "## Criterion\n",
    "\n",
    "The loss function is a weighted sum of the response loss regularized by the response time loss. The regularizer weight is controlled by the `time_loss_beta` hyper-parameter.\n",
    "\n",
    "$L = L_{\\text{response}} + \\beta L_{\\text{time}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\n",
    "from src.cogponder import CogPonderModel\n",
    "from src.cogponder.datasets import StroopSRODataset, CogPonderDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset and configs\n",
    "\n",
    "print('Loading Stroop dataset... ', end='')\n",
    "\n",
    "data = StroopSRODataset(n_subjects=1)\n",
    "\n",
    "# parameter space\n",
    "CONFIG = {\n",
    "    'task': 'stroop',\n",
    "    'checkpoint_path': 'models/stroop/fixed_hp.ckpt',\n",
    "    'resp_loss_beta': 1.,\n",
    "    'time_loss_beta': 4.,\n",
    "    'non_decision_time': 10,  # in milliseconds\n",
    "    'loss_by_trial_type': False,\n",
    "    'learning_rate': 1e-2,\n",
    "    'max_response_step': data[0][4].max() + 1,\n",
    "    'inputs_dim': data[0][0].size(1),\n",
    "    'embeddings_dim': 2,\n",
    "    'outputs_dim': torch.unique(data[0][3]).size(0),  # number of unique responses\n",
    "    'auto_lr_find': False,\n",
    "    'batch_size': 196,\n",
    "}\n",
    "\n",
    "datamodule = CogPonderDataModule(data, batch_size=CONFIG['batch_size'], num_workers=8)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Define the pondering model and run the trainer\n",
    "\n",
    "model = CogPonderModel(CONFIG, example_input_array=data[0][0][:1].to(device))\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10000,\n",
    "    # min_epochs=100,\n",
    "    accelerator='auto',\n",
    "    auto_lr_find=CONFIG['auto_lr_find'],\n",
    "    log_every_n_steps=1,\n",
    "    # overfit_batches=True,\n",
    "    # accumulate_grad_batches=2,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor='val/total_loss', patience=1000, mode='min', min_delta=0.001),\n",
    "    ])\n",
    "\n",
    "# Auto-detect learning-rate if the flag is set\n",
    "if CONFIG['auto_lr_find']:\n",
    "    trainer.tune(model, datamodule=datamodule)\n",
    "\n",
    "# Fit ans evaluate the model\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "# Save the checkpoint\n",
    "trainer.save_checkpoint(CONFIG['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "# DEBUG - Load the checkpoint\n",
    "\n",
    "model_ckpt = CogPonderModel.load_from_checkpoint(CONFIG['checkpoint_path'])\n",
    "model_ckpt.eval()\n",
    "\n",
    "if not 'datamodule' in locals() or not hasattr(datamodule, 'train_dataset'):\n",
    "    print('loading data module...')\n",
    "    datamodule = CogPonderDataModule(data, batch_size=CONFIG['batch_size'], num_workers=8)\n",
    "    datamodule.prepare_data()\n",
    "\n",
    "X_train, trial_types_train, _, y_train, rt_train = datamodule.dataset[datamodule.train_dataset.indices]\n",
    "X_test, trial_types_test, _, y_test, rt_test = datamodule.dataset[datamodule.test_dataset.indices]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_ckpt.eval()\n",
    "\n",
    "    y_train_steps,_,rt_train_pred = model_ckpt(X_train)\n",
    "    y_test_steps,_,rt_test_pred = model_ckpt(X_test)\n",
    "    \n",
    "    y_train_steps = torch.argmax(y_train_steps, dim=-1)\n",
    "    y_test_steps = torch.argmax(y_test_steps, dim=-1)\n",
    "\n",
    "    y_train_pred = y_train_steps.gather(dim=0, index=rt_train_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "    y_test_pred = y_test_steps.gather(dim=0, index=rt_test_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "\n",
    "    train_res = pd.DataFrame({'true_rt_train': rt_train.detach().tolist(),\n",
    "                              'pred_rt_train': rt_train_pred.tolist()})\n",
    "    test_res = pd.DataFrame({'true_rt_test': rt_test.detach().tolist(),\n",
    "                             'pred_rt_test': rt_test_pred.tolist()})\n",
    "\n",
    "    display(train_res.T, test_res.T)\n",
    "\n",
    "    # report accuracy\n",
    "    match CONFIG['task']:\n",
    "        case 'nback':\n",
    "            accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "            train_accuracy = accuracy_fn(y_train_pred, y_train.int())\n",
    "            print('NBACK TRAIN ACCURACY', train_accuracy.item())\n",
    "            accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "            test_accuracy = accuracy_fn(y_test_pred, y_test.int())\n",
    "            print('NBACK TEST  ACCURACY', test_accuracy.item())\n",
    "        case 'stroop':\n",
    "            pass\n",
    "\n",
    "# DEBUG report medians\n",
    "print(f'RT train mean (pred/true): {rt_train_pred.float().mean().item():.2f}, {rt_train.float().mean().item():.2f}')\n",
    "print(f'RT test  mean (pred/true): {rt_test_pred.float().mean().item():.2f}, {rt_test.float().mean().item():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "plot_data_train = pd.DataFrame({\n",
    "    'rt': rt_train,\n",
    "    'trial_type': trial_types_train,\n",
    "    'color': X_train[:, 0],\n",
    "    'word': X_train[:, 1],\n",
    "    'data': 'truth',\n",
    "    'split': 'train',\n",
    "})\n",
    "\n",
    "plot_data_train_pred = pd.DataFrame({\n",
    "    'rt': rt_train_pred,\n",
    "    'trial_type': trial_types_train,\n",
    "    'color': X_train[:, 0],\n",
    "    'word': X_train[:, 1],\n",
    "    'data': 'pred',\n",
    "    'split': 'train',\n",
    "})\n",
    "\n",
    "plot_data_test = pd.DataFrame({\n",
    "    'rt': rt_test,\n",
    "    'trial_type': trial_types_test,\n",
    "    'color': X_test[:, 0],\n",
    "    'word': X_test[:, 1],\n",
    "    'data': 'truth',\n",
    "    'split': 'test',\n",
    "})\n",
    "\n",
    "plot_data_test_pred = pd.DataFrame({\n",
    "    'rt': rt_test_pred,\n",
    "    'trial_type': trial_types_test,\n",
    "    'color': X_test[:, 0],\n",
    "    'word': X_test[:, 1],\n",
    "    'data': 'pred',\n",
    "    'split': 'test',\n",
    "})\n",
    "\n",
    "# merge and cleanup\n",
    "plot_data = pd.concat([plot_data_train, plot_data_train_pred, plot_data_test, plot_data_test_pred])\n",
    "plot_data['trial_type'] = plot_data['trial_type'].map({0: 'incongruent', 1: 'congruent'})\n",
    "\n",
    "max_rt = CONFIG[\"max_response_step\"].item()\n",
    "\n",
    "plot_data = plot_data.query('rt < @max_rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = sns.displot(\n",
    "    plot_data,\n",
    "    x='rt', row='data', col='split', hue='trial_type',\n",
    "    kind='ecdf', linewidth=2, height=3,\n",
    ")\n",
    "\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# plt.title(f'Evaluation profile of CogPonder on train split (Stroop)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.displot(\n",
    "    plot_data,\n",
    "    x='rt', row='data', col='split', #hue='trial_type',\n",
    "    kind='kde', linewidth=2, height=3, #palette='Greens',\n",
    "    clip=(0, max_rt),\n",
    ")\n",
    "\n",
    "# sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "# plt.title(f'Evaluation profile of CogPonder on train split (Stroop)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ponder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76027f054aa38c6d5055560f9009a0f692d2c97ebd1740bbb514e2260aad097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
