{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogPonder: Stroop task with fixed hyper-parameters\n",
    "\n",
    "This notebook implements the CogPonder framework using PyTorch Lightning and evaluate it on the Stroop task.\n",
    "\n",
    "\n",
    "It assumes fixed hyper-parameters and fits the model to a single-subject dataset. It wraps a simple linear network with a pondering layer and trains it on the *Self-Regulation Ontology* dataset.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "Here, we fit the Stroop data from the *Self-Regulation Ontology*. The data is loaded from the `data/Self_Regulation_ontology/` directory; see the [README](../data/Self_Regulation_Ontology/README.md) for more information on the data structure.\n",
    "\n",
    "### Input and Output\n",
    "\n",
    "\n",
    "The input contains a list of trials, each includes 1) the color and 2) the letter of the current stimulus. The output is the human response to the current trial (i.e., red, green, or blue).\n",
    "\n",
    "\n",
    "## Hyper-parameters\n",
    "\n",
    "- `n_embeddings`: number of hidden units in the operator model. Defaults to 2.\n",
    "- `resp_loss_beta`: the beta parameter of the loss function. Defaults to 1.\n",
    "- `time_loss_beta`: the beta parameter of the loss function. Defaults to 0.5.\n",
    "- `learning_rate`: the learning rate of the optimizer. Defaults to 0.0001.\n",
    "- `max_response_step`: maximum response step in the dataset. Defaults to 100.\n",
    "\n",
    "## Criterion\n",
    "\n",
    "The loss function is a weighted sum of the response loss regularized by the response time loss. The regularizer weight is controlled by the `time_loss_beta` hyper-parameter.\n",
    "\n",
    "$L = L_{\\text{response}} + \\beta L_{\\text{time}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\n",
    "from src.cogponder import CogPonderModel\n",
    "from src.cogponder.datasets import StroopSRODataset, CogPonderDataModule\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# this notebook only fits one SRO subject, which its SRO-SubjectID can be defined here\n",
    "SRO_SUBJECT_ID = 202\n",
    "\n",
    "# number of maximum epochs to train\n",
    "MAX_EPOCHS = 10000\n",
    "\n",
    "# upon successful training, the model will be saved to this path\n",
    "CHECKPOINT_PATH = Path('models/stroop/') / f'cogponder_subject-{SRO_SUBJECT_ID}_epochs-{MAX_EPOCHS}.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stroop dataset... Done!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and configs\n",
    "\n",
    "print('Loading Stroop dataset... ', end='')\n",
    "\n",
    "data = StroopSRODataset(n_subjects=-1, response_step_interval=10, non_decision_time='auto')[SRO_SUBJECT_ID]\n",
    "\n",
    "# determine the number of loaded subjects\n",
    "n_subjects = data[:][0].size(1)\n",
    "\n",
    "# parameter space\n",
    "CONFIG = {\n",
    "    'task': 'stroop',\n",
    "    'resp_loss_beta': 1.,\n",
    "    'time_loss_beta': 10.,\n",
    "    # 'non_decision_time': 10,  # in milliseconds\n",
    "    'loss_by_trial_type': False,\n",
    "    'learning_rate': 1e-2,\n",
    "    'max_response_step': data[4].max().int().item() + 10,\n",
    "    'inputs_dim': data[0].size(1) - 1,  # minus subject_id (first column)\n",
    "    'embeddings_dim': 2,\n",
    "    'outputs_dim': torch.unique(data[3]).size(0),  # number of unique responses\n",
    "    'auto_lr_find': False,\n",
    "    'batch_size': 72,\n",
    "    'n_subjects': 1\n",
    "}\n",
    "\n",
    "datamodule = CogPonderDataModule(data,\n",
    "                                 batch_size=CONFIG['batch_size'],\n",
    "                                 num_workers=8)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ train_accuracy    │ Accuracy         │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_accuracy      │ Accuracy         │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ resp_loss_fn      │ ResponseLoss     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ time_loss_fn      │ ResponseTimeLoss │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ halt_node         │ Sequential       │      9 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ output_node       │ Sequential       │     15 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ recurrent_node    │ GRUCell          │     36 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>│ subject_embedding │ Embedding        │      2 │\n",
       "└───┴───────────────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ train_accuracy    │ Accuracy         │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_accuracy      │ Accuracy         │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ resp_loss_fn      │ ResponseLoss     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ time_loss_fn      │ ResponseTimeLoss │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ halt_node         │ Sequential       │      9 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ output_node       │ Sequential       │     15 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ recurrent_node    │ GRUCell          │     36 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ subject_embedding │ Embedding        │      2 │\n",
       "└───┴───────────────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 62                                                                                               \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 62                                                                                                   \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 62                                                                                               \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 62                                                                                                   \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb8ecaef0574f2e9590f0c04054990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the pondering model and run the trainer\n",
    "\n",
    "model = CogPonderModel(CONFIG)#, example_input_array=data[0][:1].to(device))\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    # min_epochs=100,\n",
    "    accelerator='auto',\n",
    "    auto_lr_find=CONFIG['auto_lr_find'],\n",
    "    log_every_n_steps=1,\n",
    "    # overfit_batches=True,\n",
    "    # accumulate_grad_batches=2,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor='val/total_loss',\n",
    "                      patience=np.max([10, MAX_EPOCHS // 10]).item(),\n",
    "                      mode='min', min_delta=0.001),\n",
    "    ])\n",
    "\n",
    "# Auto-detect learning-rate if the flag is set\n",
    "if CONFIG['auto_lr_find']:\n",
    "    trainer.tune(model, datamodule=datamodule)\n",
    "\n",
    "# Fit and evaluate the model\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "# Save the latest checkpoint\n",
    "trainer.save_checkpoint(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "# DEBUG - Load the checkpoint\n",
    "\n",
    "model_ckpt = CogPonderModel.load_from_checkpoint(CHECKPOINT_PATH)\n",
    "model_ckpt.eval()\n",
    "\n",
    "if not 'datamodule' in locals() or not hasattr(datamodule, 'train_dataset'):\n",
    "    print('loading data module...', end='')\n",
    "    data = StroopSRODataset(n_subjects=-1, response_step_interval=10)[SRO_SUBJECT_ID]\n",
    "    datamodule = CogPonderDataModule(data, batch_size=CONFIG['batch_size'], num_workers=8)\n",
    "    datamodule.prepare_data()\n",
    "    print('Done!')\n",
    "\n",
    "X_train, trial_types_train, is_corrects_train, y_train, rt_train = datamodule.train_dataset[:]\n",
    "X_test, trial_types_test, is_corrects_test, y_test, rt_test = datamodule.test_dataset[:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_ckpt.eval()\n",
    "\n",
    "    y_train_steps,p_train,rt_train_pred = model_ckpt(X_train)\n",
    "    y_test_steps,p_test,rt_test_pred = model_ckpt(X_test)\n",
    "\n",
    "    # print(p_train.T[0].shape)\n",
    "    # print(p_train.T[0])\n",
    "\n",
    "\n",
    "\n",
    "    y_train_steps = torch.argmax(y_train_steps, dim=-1)\n",
    "    y_test_steps = torch.argmax(y_test_steps, dim=-1)\n",
    "\n",
    "    y_train_pred = y_train_steps.gather(dim=0, index=rt_train_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "    y_test_pred = y_test_steps.gather(dim=0, index=rt_test_pred[None, :] - 1,)[0]  # (batch_size,)\n",
    "\n",
    "    train_res = pd.DataFrame({'true_rt_train': rt_train.detach().tolist(),\n",
    "                              'pred_rt_train': rt_train_pred.tolist()})\n",
    "    test_res = pd.DataFrame({'true_rt_test': rt_test.detach().tolist(),\n",
    "                             'pred_rt_test': rt_test_pred.tolist()})\n",
    "\n",
    "    display(train_res.T, test_res.T)\n",
    "\n",
    "    # report accuracy\n",
    "    match CONFIG['task']:\n",
    "        case 'nback':\n",
    "            accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "            train_accuracy = accuracy_fn(y_train_pred, y_train.int())\n",
    "            print('NBACK TRAIN ACCURACY', train_accuracy.item())\n",
    "            accuracy_fn = torchmetrics.Accuracy().to(device)\n",
    "            test_accuracy = accuracy_fn(y_test_pred, y_test.int())\n",
    "            print('NBACK TEST  ACCURACY', test_accuracy.item())\n",
    "        case 'stroop':\n",
    "            pass\n",
    "\n",
    "# DEBUG report medians\n",
    "print(f'RT train mean (pred/true): '\n",
    "      f'{rt_train_pred.float().mean().item():.2f}, '\n",
    "      f'{rt_train.float().mean().item():.2f}')\n",
    "\n",
    "print(f'RT test  mean (pred/true): '\n",
    "      f'{rt_test_pred.float().mean().item():.2f}, '\n",
    "      f'{rt_test.float().mean().item():.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot CogPonder accuracy (single subject)\n",
    "\n",
    "cong_train_corrects = np.where(trial_types_train == 1, y_train_pred == y_train, np.nan)\n",
    "incong_train_corrects = np.where(trial_types_train == 0, y_train_pred == y_train, np.nan)\n",
    "\n",
    "cong_test_corrects = np.where(trial_types_test == 1, y_test_pred == y_test, np.nan)\n",
    "incong_test_corrects = np.where(trial_types_test == 0, y_test_pred == y_test, np.nan)\n",
    "\n",
    "human_cong_train_corrects = np.where(trial_types_train == 1, is_corrects_train, np.nan)\n",
    "human_incong_train_corrects = np.where(trial_types_train == 0, is_corrects_train, np.nan)\n",
    "\n",
    "human_cong_test_corrects = np.where(trial_types_test == 1, is_corrects_test, np.nan)\n",
    "human_incong_test_corrects = np.where(trial_types_test == 0, is_corrects_test, np.nan)\n",
    "\n",
    "plot_data = pd.DataFrame({\n",
    "    ('CogPonder', 'congruent', 'train', np.nanmean(cong_train_corrects)),\n",
    "    ('CogPonder', 'incongruent', 'train', np.nanmean(incong_train_corrects)),\n",
    "    ('CogPonder', 'congruent', 'test', np.nanmean(cong_test_corrects)),\n",
    "    ('CogPonder', 'incongruent', 'test', np.nanmean(incong_test_corrects)),\n",
    "    ('Human', 'congruent', 'train', np.nanmean(human_cong_train_corrects)),\n",
    "    ('Human', 'incongruent', 'train', np.nanmean(human_incong_train_corrects)),\n",
    "    ('Human', 'congruent', 'test', np.nanmean(human_cong_test_corrects)),\n",
    "    ('Human', 'incongruent', 'test', np.nanmean(human_incong_test_corrects)),\n",
    "\n",
    "}, columns=['dataset', 'condition', 'split', 'accuracy'])\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    data=plot_data, col='dataset',\n",
    "    palette='Blues',\n",
    "    col_order=['Human', 'CogPonder'],\n",
    "    sharey=True, sharex=False, height=3, aspect=1)\n",
    "\n",
    "g.map(sns.barplot, 'split', 'accuracy', 'condition',\n",
    "      width=.3, order=['train', 'test'],\n",
    "      hue_order=['congruent', 'incongruent'],\n",
    "      palette='Set2')\n",
    "\n",
    "g.add_legend(loc='upper left', bbox_to_anchor=(.95, .8))\n",
    "\n",
    "plt.suptitle('Stroop accuracy (N=1)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/cogponder_stroop_accuracy.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# prepare plotting data\n",
    "\n",
    "plot_data_train = pd.DataFrame({\n",
    "    'rt': rt_train,\n",
    "    'trial_type': trial_types_train,\n",
    "    'color': X_train[:, 0],\n",
    "    'word': X_train[:, 1],\n",
    "    'data': 'truth',\n",
    "    'split': 'train',\n",
    "})\n",
    "\n",
    "plot_data_train_pred = pd.DataFrame({\n",
    "    'rt': rt_train_pred,\n",
    "    'trial_type': trial_types_train,\n",
    "    'color': X_train[:, 0],\n",
    "    'word': X_train[:, 1],\n",
    "    'data': 'pred',\n",
    "    'split': 'train',\n",
    "})\n",
    "\n",
    "plot_data_test = pd.DataFrame({\n",
    "    'rt': rt_test,\n",
    "    'trial_type': trial_types_test,\n",
    "    'color': X_test[:, 0],\n",
    "    'word': X_test[:, 1],\n",
    "    'data': 'truth',\n",
    "    'split': 'test',\n",
    "})\n",
    "\n",
    "plot_data_test_pred = pd.DataFrame({\n",
    "    'rt': rt_test_pred,\n",
    "    'trial_type': trial_types_test,\n",
    "    'color': X_test[:, 0],\n",
    "    'word': X_test[:, 1],\n",
    "    'data': 'pred',\n",
    "    'split': 'test',\n",
    "})\n",
    "\n",
    "# merge and cleanup\n",
    "plot_data = pd.concat([plot_data_train, plot_data_train_pred, plot_data_test, plot_data_test_pred])\n",
    "plot_data['trial_type'] = plot_data['trial_type'].map({0: 'incongruent', 1: 'congruent'})\n",
    "\n",
    "max_rt = CONFIG[\"max_response_step\"]\n",
    "\n",
    "plot_data = plot_data.query('rt < @max_rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "g = sns.displot(\n",
    "    plot_data,\n",
    "    x='rt', row='data', col='split', hue='trial_type',\n",
    "    kind='ecdf', linewidth=2, height=3,\n",
    ")\n",
    "\n",
    "sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "# plt.title(f'Evaluation profile of CogPonder on train split (Stroop)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.displot(\n",
    "    plot_data.reset_index(),\n",
    "    x='rt', row='data', col='split', #hue='trial_type',\n",
    "    kind='hist', height=3, #palette='Greens',\n",
    "    kde=True, kde_kws={'clip': (0, max_rt)},\n",
    "    facet_kws={'sharey': False}\n",
    ")\n",
    "# sns.move_legend(g, 'upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "# plt.title(f'Evaluation profile of CogPonder on train split (Stroop)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ponder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70f45b4cee24464c60504a5fe56f777ef44c770208b66a1d7d40e1cf1ca2ecf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
